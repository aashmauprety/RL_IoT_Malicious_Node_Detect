{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL_IoT_Malicious(Final).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClmnNw5h6Tp"
      },
      "source": [
        "!pip install keras-rl2\n",
        "!pip install keras\n",
        "!pip install tensorflow==2.3.0\n",
        "!pip install gym"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx8bKta1i_jh"
      },
      "source": [
        "# import env class from gym to create custom environment\n",
        "from gym import Env\n",
        "\n",
        "# import two spaces from gym, allow us to define state and actions\n",
        "from gym.spaces import Discrete, Box  \n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import random"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84KtvQ9WjHA_",
        "outputId": "14423ee9-b039-4201-8421-2a52bbe5c24b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCidYGAnjV62"
      },
      "source": [
        "Read Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kbcC3CmjQUc"
      },
      "source": [
        "data = pd.read_csv(\"/content/drive/My Drive/sample_data.csv\")\n",
        "weight = pd.DataFrame(data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlHYjjA_jfEm"
      },
      "source": [
        "Simulate index for two malicious and three non-malicious clients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhY_N9vejZfW"
      },
      "source": [
        "malicious_index = [0,1]\n",
        "non_malicious_index = [2,3,4]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdfre1DNjqRx"
      },
      "source": [
        "Define state, action and reward for RL agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8m4QRu2jYPg"
      },
      "source": [
        "class findmeEnv(Env):\n",
        "    STATE_ELEMENTS = 5\n",
        "\n",
        "    def __init__(self, weight):\n",
        "        self.weight = weight\n",
        "        self.action_space = Discrete(5)\n",
        "        self.observation_space = Box(low=0, high=2, shape=(findmeEnv.STATE_ELEMENTS,)) #10 comm.round, 5 clients\n",
        "        self.state = self.weight.iloc[0:1].values\n",
        "        self.state1 = self.weight.iloc[0:1]\n",
        "        self.length = 100\n",
        "\n",
        "\n",
        "\n",
        "    def step(self,action):\n",
        "        if action==0:   #reward for this action is +2\n",
        "            a = random.choice(malicious_index)\n",
        "            self.state1.iloc[:,a].replace([self.state1.iloc[:,a]],0,inplace=True)\n",
        "            \n",
        "        if action==1: #reward for this action is -10\n",
        "            b = random.choice(non_malicious_index)\n",
        "            self.state1.iloc[:,b].replace([self.state1.iloc[:,b]],0,inplace=True)\n",
        "  \n",
        "\n",
        "        if action==2: #reward is +5\n",
        "            self.state1.iloc[:,0].replace([self.state1.iloc[:,0]],0,inplace=True)\n",
        "            self.state1.iloc[:,1].replace([self.state1.iloc[:,1]],0,inplace=True)\n",
        "            \n",
        "\n",
        "        if action==3: #reward is +1\n",
        "            x = random.choice(malicious_index)\n",
        "            y = random.choice(non_malicious_index)\n",
        "            self.state1.iloc[:,x].replace([self.state1.iloc[:,x]],0,inplace=True)\n",
        "            self.state1.iloc[:,y].replace([self.state1.iloc[:,y]],0,inplace=True)\n",
        "            \n",
        "\n",
        "\n",
        "        if action==4: #reward is -15\n",
        "            self.state1.iloc[:,2].replace([self.state1.iloc[:,2]],0,inplace=True)\n",
        "            self.state1.iloc[:,3].replace([self.state1.iloc[:,3]],0,inplace=True)\n",
        "            self.state1.iloc[:,4].replace([self.state1.iloc[:,4]],0,inplace=True)\n",
        "\n",
        "        self.length -= 1\n",
        "\n",
        "\n",
        "        # Defind reward value \n",
        "        if action==0:\n",
        "            reward = 2\n",
        "        if action==1:\n",
        "            reward = -10\n",
        "        if action == 2:\n",
        "            reward = 5\n",
        "        if action==3:\n",
        "            reward = 1\n",
        "        if action==4:\n",
        "            reward = -15\n",
        "\n",
        "\n",
        "        if self.length <= 0: \n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        info = {}\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.weight.sample().reset_index(drop = True).values\n",
        "        self.state = np.squeeze(self.state, axis=0)\n",
        "        self.length = 100\n",
        "        return self.state\n",
        "        "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIL6Ok39kiIZ"
      },
      "source": [
        "Instantiate the RL environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0L2D2pSKkWlJ"
      },
      "source": [
        "env = findmeEnv(weight)\n",
        "# obs = env.reset()\n",
        "# print(obs.shape)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDjOQ5N9ko24"
      },
      "source": [
        "Get shape of state and action"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we3tt6hFkk2P",
        "outputId": "cb0ceda0-fea0-416e-fa19-642b9d23f2ad"
      },
      "source": [
        "states = env.observation_space.shape \n",
        "actions = env.action_space.n \n",
        "actions \n",
        "states"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5,)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbT9HJ8Aky9U"
      },
      "source": [
        "**CREATE A DEEP LEARNING MODEL WITH KERAS**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Gx290HkxDq"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# from tensorflow.keras.callbacks import TensorBoard\n",
        "import numpy as np\n",
        "import random \n",
        "import time"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slk46ofKlCxi"
      },
      "source": [
        "**Define model architecture**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbe5meKZkuCj"
      },
      "source": [
        "def build_model(states,actions):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(64, activation='relu', input_shape = (1,5)))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dense(actions, activation='linear'))\n",
        "    model.add(Flatten())\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41-32L0rlGUg"
      },
      "source": [
        "model = build_model(states,actions)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1isINoMhlQXi"
      },
      "source": [
        "**Define DQN Agent and train it on the env**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Ylg5z8lKO9"
      },
      "source": [
        "from rl.agents import DQNAgent\n",
        "from rl.policy import BoltzmannQPolicy\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.callbacks import ModelIntervalCheckpoint, FileLogger\n",
        "\n",
        "\n",
        "def build_agent(model, actions):\n",
        "  policy = BoltzmannQPolicy()\n",
        "  memory = SequentialMemory(limit=50000, window_length = 1)\n",
        "  dqn = DQNAgent(model = model, memory = memory, policy=policy, nb_actions=actions, nb_steps_warmup = 10, target_model_update = 1e-2)\n",
        "  return dqn\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR6667_wlqQl"
      },
      "source": [
        "**Start training the agent on the given environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXNGnzilyVaz"
      },
      "source": [
        "ENV_NAME = 'RL_IoT-v1'"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrEVA-TSzRst"
      },
      "source": [
        "def build_callbacks(ENV_NAME):\n",
        "    checkpoint_weights_filename = 'dqn_' + ENV_NAME + '_weights_{step}.h5f'\n",
        "    log_filename = 'dqn_{}_log.json'.format(ENV_NAME)\n",
        "    callbacks = [ModelIntervalCheckpoint(checkpoint_weights_filename, interval=250000)]\n",
        "    callbacks += [FileLogger(log_filename, interval=100)]\n",
        "    return callbacks"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXvpzxe-0Ptd"
      },
      "source": [
        "policy = BoltzmannQPolicy()\n",
        "memory = SequentialMemory(limit=50000, window_length = 1)\n",
        "dqn = DQNAgent(model=model, nb_actions=actions, policy=policy, memory=memory, nb_steps_warmup=50000, gamma=.99, target_model_update=10000,\n",
        "                   train_interval=4, delta_clip=1.)\n",
        "dqn.compile(Adam(lr=.00025), metrics=['mae'])\n",
        "callbacks = build_callbacks(ENV_NAME)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9scLj8ot0tlN",
        "outputId": "5c3a4ff9-3bdb-490d-8f5d-123ce087d96e"
      },
      "source": [
        "dqn.fit(env, nb_steps=100)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 100 steps ...\n",
            "Interval 1 (0 steps performed)\n",
            "   94/10000 [..............................] - ETA: 22s - reward: 4.2979done, took 0.234 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f6b3821af10>"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQsHj35MlbW8",
        "outputId": "0c135daf-6997-4f0d-c3e3-c01ee7d8f7de"
      },
      "source": [
        "dqn = build_agent(model, actions)\n",
        "dqn.compile(Adam(learning_rate=1e-3), metrics=['mae'])\n",
        "callbacks = build_callbacks(ENV_NAME)\n",
        "dqn.fit(env, nb_steps=20000, visualize=False,verbose=2, callbacks=callbacks)\n",
        "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training for 20000 steps ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/rl/memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
            "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   100/20000: episode: 1, duration: 7.463s, episode steps: 100, steps per second:  13, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 0.111432, mae: 124.998340, mean_q: 261.242375\n",
            "   200/20000: episode: 2, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: 455.000, mean reward:  4.550 [ 2.000,  5.000], mean action: 1.700 [0.000, 2.000],  loss: 198.838531, mae: 123.718750, mean_q: 256.469513\n",
            "   300/20000: episode: 3, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: 488.000, mean reward:  4.880 [ 2.000,  5.000], mean action: 1.920 [0.000, 2.000],  loss: 252.117752, mae: 122.723282, mean_q: 257.169250\n",
            "   400/20000: episode: 4, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 245.438461, mae: 120.792557, mean_q: 257.878876\n",
            "   500/20000: episode: 5, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 304.119659, mae: 118.783623, mean_q: 255.560013\n",
            "   600/20000: episode: 6, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 260.301208, mae: 121.829445, mean_q: 258.250427\n",
            "   700/20000: episode: 7, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 311.943054, mae: 123.525284, mean_q: 259.864136\n",
            "   800/20000: episode: 8, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 378.103790, mae: 122.243233, mean_q: 257.999268\n",
            "   900/20000: episode: 9, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 370.058594, mae: 120.706299, mean_q: 256.056549\n",
            "  1000/20000: episode: 10, duration: 1.187s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 333.407318, mae: 119.140602, mean_q: 253.671265\n",
            "  1100/20000: episode: 11, duration: 1.182s, episode steps: 100, steps per second:  85, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 359.618713, mae: 118.731377, mean_q: 251.354477\n",
            "  1200/20000: episode: 12, duration: 1.227s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 314.829132, mae: 118.578194, mean_q: 250.985336\n",
            "  1300/20000: episode: 13, duration: 1.275s, episode steps: 100, steps per second:  78, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 332.391632, mae: 118.291107, mean_q: 251.100632\n",
            "  1400/20000: episode: 14, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 244.210159, mae: 119.089561, mean_q: 252.291855\n",
            "  1500/20000: episode: 15, duration: 1.202s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 319.407837, mae: 118.217987, mean_q: 251.382095\n",
            "  1600/20000: episode: 16, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 359.242218, mae: 117.370895, mean_q: 249.539841\n",
            "  1700/20000: episode: 17, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 244.214981, mae: 117.419319, mean_q: 250.716431\n",
            "  1800/20000: episode: 18, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 217.403748, mae: 119.006882, mean_q: 252.802338\n",
            "  1900/20000: episode: 19, duration: 1.242s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 242.858109, mae: 120.062279, mean_q: 253.768784\n",
            "  2000/20000: episode: 20, duration: 1.264s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 207.039337, mae: 118.896843, mean_q: 254.945755\n",
            "  2100/20000: episode: 21, duration: 1.232s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 281.137787, mae: 118.515068, mean_q: 255.672638\n",
            "  2200/20000: episode: 22, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 248.331802, mae: 119.149269, mean_q: 256.274872\n",
            "  2300/20000: episode: 23, duration: 1.275s, episode steps: 100, steps per second:  78, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 234.778320, mae: 120.421120, mean_q: 258.252411\n",
            "  2400/20000: episode: 24, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 323.797974, mae: 120.853294, mean_q: 257.657990\n",
            "  2500/20000: episode: 25, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 270.459839, mae: 120.980362, mean_q: 259.537109\n",
            "  2600/20000: episode: 26, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 337.538879, mae: 121.882301, mean_q: 258.855560\n",
            "  2700/20000: episode: 27, duration: 1.219s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 314.194183, mae: 121.748444, mean_q: 259.597229\n",
            "  2800/20000: episode: 28, duration: 1.226s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 362.757507, mae: 122.559319, mean_q: 259.139740\n",
            "  2900/20000: episode: 29, duration: 1.231s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 402.869171, mae: 122.709305, mean_q: 258.897491\n",
            "  3000/20000: episode: 30, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 291.898438, mae: 122.981445, mean_q: 259.969086\n",
            "  3100/20000: episode: 31, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 288.097961, mae: 123.878731, mean_q: 261.009369\n",
            "  3200/20000: episode: 32, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 394.437866, mae: 123.076637, mean_q: 260.250000\n",
            "  3300/20000: episode: 33, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 245.770203, mae: 122.802872, mean_q: 262.128540\n",
            "  3400/20000: episode: 34, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 307.799347, mae: 123.007103, mean_q: 262.487549\n",
            "  3500/20000: episode: 35, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 322.738098, mae: 121.593201, mean_q: 262.399994\n",
            "  3600/20000: episode: 36, duration: 1.255s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 310.589844, mae: 122.044464, mean_q: 262.226929\n",
            "  3700/20000: episode: 37, duration: 1.294s, episode steps: 100, steps per second:  77, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 360.889496, mae: 123.384338, mean_q: 261.700226\n",
            "  3800/20000: episode: 38, duration: 1.257s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 301.793518, mae: 123.181709, mean_q: 261.623657\n",
            "  3900/20000: episode: 39, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 321.436157, mae: 122.968819, mean_q: 260.881653\n",
            "  4000/20000: episode: 40, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 311.177155, mae: 123.546349, mean_q: 261.884888\n",
            "  4100/20000: episode: 41, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 336.877655, mae: 122.716911, mean_q: 259.963989\n",
            "  4200/20000: episode: 42, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 290.870483, mae: 123.281570, mean_q: 260.556519\n",
            "  4300/20000: episode: 43, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 301.525085, mae: 122.703934, mean_q: 261.399658\n",
            "  4400/20000: episode: 44, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 285.371185, mae: 121.237732, mean_q: 261.775696\n",
            "  4500/20000: episode: 45, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 263.214813, mae: 122.688278, mean_q: 262.391693\n",
            "  4600/20000: episode: 46, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 404.688599, mae: 122.742004, mean_q: 260.877777\n",
            "  4700/20000: episode: 47, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 382.541565, mae: 122.755768, mean_q: 259.825928\n",
            "  4800/20000: episode: 48, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 192.035217, mae: 123.162529, mean_q: 261.086914\n",
            "  4900/20000: episode: 49, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 299.112213, mae: 123.197517, mean_q: 260.998566\n",
            "  5000/20000: episode: 50, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 255.432678, mae: 123.494659, mean_q: 261.981415\n",
            "  5100/20000: episode: 51, duration: 1.224s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 384.048798, mae: 124.205978, mean_q: 260.910828\n",
            "  5200/20000: episode: 52, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: 497.000, mean reward:  4.970 [ 2.000,  5.000], mean action: 1.980 [0.000, 2.000],  loss: 344.653503, mae: 124.586433, mean_q: 261.242554\n",
            "  5300/20000: episode: 53, duration: 1.231s, episode steps: 100, steps per second:  81, episode reward: 491.000, mean reward:  4.910 [ 2.000,  5.000], mean action: 1.940 [0.000, 2.000],  loss: 372.105774, mae: 124.308289, mean_q: 259.979248\n",
            "  5400/20000: episode: 54, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 384.733093, mae: 122.424690, mean_q: 260.018585\n",
            "  5500/20000: episode: 55, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 269.062805, mae: 122.316559, mean_q: 261.564911\n",
            "  5600/20000: episode: 56, duration: 1.196s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 419.588043, mae: 123.014313, mean_q: 260.546478\n",
            "  5700/20000: episode: 57, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 333.243347, mae: 122.107933, mean_q: 260.517944\n",
            "  5800/20000: episode: 58, duration: 1.195s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 390.096375, mae: 119.546570, mean_q: 258.943542\n",
            "  5900/20000: episode: 59, duration: 1.189s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 297.826843, mae: 119.994118, mean_q: 259.801849\n",
            "  6000/20000: episode: 60, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 319.092651, mae: 121.212448, mean_q: 258.364258\n",
            "  6100/20000: episode: 61, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 389.312378, mae: 121.909714, mean_q: 257.433380\n",
            "  6200/20000: episode: 62, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 336.475555, mae: 122.374802, mean_q: 258.985474\n",
            "  6300/20000: episode: 63, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 282.643372, mae: 120.908577, mean_q: 258.160095\n",
            "  6400/20000: episode: 64, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 384.566772, mae: 121.749588, mean_q: 258.427277\n",
            "  6500/20000: episode: 65, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 369.301392, mae: 119.207832, mean_q: 256.436951\n",
            "  6600/20000: episode: 66, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 356.197388, mae: 117.661530, mean_q: 256.723755\n",
            "  6700/20000: episode: 67, duration: 1.243s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 380.874481, mae: 117.809860, mean_q: 255.805328\n",
            "  6800/20000: episode: 68, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 325.411530, mae: 117.367302, mean_q: 255.685455\n",
            "  6900/20000: episode: 69, duration: 1.243s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 340.729492, mae: 114.508720, mean_q: 255.601913\n",
            "  7000/20000: episode: 70, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 245.163651, mae: 115.693001, mean_q: 256.694000\n",
            "  7100/20000: episode: 71, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 260.996399, mae: 117.337898, mean_q: 256.317749\n",
            "  7200/20000: episode: 72, duration: 1.227s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 348.788696, mae: 116.380325, mean_q: 255.769836\n",
            "  7300/20000: episode: 73, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 271.567230, mae: 116.849541, mean_q: 256.394623\n",
            "  7400/20000: episode: 74, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 344.211884, mae: 118.700150, mean_q: 255.982834\n",
            "  7500/20000: episode: 75, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 352.745667, mae: 119.515625, mean_q: 255.535828\n",
            "  7600/20000: episode: 76, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 375.767151, mae: 119.850800, mean_q: 254.848373\n",
            "  7700/20000: episode: 77, duration: 1.256s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 335.709106, mae: 118.608955, mean_q: 253.975494\n",
            "  7800/20000: episode: 78, duration: 1.260s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 338.514801, mae: 117.928497, mean_q: 254.676666\n",
            "  7900/20000: episode: 79, duration: 1.233s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 334.183411, mae: 116.935547, mean_q: 254.706192\n",
            "  8000/20000: episode: 80, duration: 1.198s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 320.844299, mae: 116.723167, mean_q: 254.997513\n",
            "  8100/20000: episode: 81, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 383.525482, mae: 116.794075, mean_q: 253.710678\n",
            "  8200/20000: episode: 82, duration: 1.203s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 327.556976, mae: 117.747726, mean_q: 253.865051\n",
            "  8300/20000: episode: 83, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 341.672394, mae: 117.490036, mean_q: 253.264954\n",
            "  8400/20000: episode: 84, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 283.292633, mae: 115.397118, mean_q: 253.873611\n",
            "  8500/20000: episode: 85, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 324.567505, mae: 115.913994, mean_q: 252.975632\n",
            "  8600/20000: episode: 86, duration: 1.229s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 376.678314, mae: 116.298210, mean_q: 252.653198\n",
            "  8700/20000: episode: 87, duration: 1.224s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 364.704651, mae: 114.397034, mean_q: 252.027344\n",
            "  8800/20000: episode: 88, duration: 1.198s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 312.188782, mae: 115.062500, mean_q: 252.038864\n",
            "  8900/20000: episode: 89, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 281.613983, mae: 116.666847, mean_q: 253.645782\n",
            "  9000/20000: episode: 90, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 334.990082, mae: 116.457733, mean_q: 252.551468\n",
            "  9100/20000: episode: 91, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 303.100220, mae: 116.886139, mean_q: 253.149109\n",
            "  9200/20000: episode: 92, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 262.582336, mae: 117.320656, mean_q: 253.539536\n",
            "  9300/20000: episode: 93, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 242.908005, mae: 116.297653, mean_q: 254.308868\n",
            "  9400/20000: episode: 94, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 282.570160, mae: 116.699699, mean_q: 254.317947\n",
            "  9500/20000: episode: 95, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 372.971161, mae: 116.914375, mean_q: 253.779694\n",
            "  9600/20000: episode: 96, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 403.622040, mae: 117.497467, mean_q: 252.190689\n",
            "  9700/20000: episode: 97, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 351.138794, mae: 117.598869, mean_q: 252.084167\n",
            "  9800/20000: episode: 98, duration: 1.239s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 263.385437, mae: 116.672295, mean_q: 252.118927\n",
            "  9900/20000: episode: 99, duration: 1.216s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 268.292084, mae: 117.910156, mean_q: 253.279343\n",
            " 10000/20000: episode: 100, duration: 1.195s, episode steps: 100, steps per second:  84, episode reward: 497.000, mean reward:  4.970 [ 2.000,  5.000], mean action: 1.980 [0.000, 2.000],  loss: 235.690903, mae: 118.113319, mean_q: 252.829895\n",
            " 10100/20000: episode: 101, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 271.475281, mae: 117.284592, mean_q: 253.038132\n",
            " 10200/20000: episode: 102, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 389.304596, mae: 115.599327, mean_q: 252.170410\n",
            " 10300/20000: episode: 103, duration: 1.231s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 346.530945, mae: 115.918762, mean_q: 251.124359\n",
            " 10400/20000: episode: 104, duration: 1.261s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 413.333710, mae: 115.343925, mean_q: 250.575104\n",
            " 10500/20000: episode: 105, duration: 1.220s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 300.507141, mae: 114.444336, mean_q: 250.046585\n",
            " 10600/20000: episode: 106, duration: 1.240s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 287.884827, mae: 114.391602, mean_q: 250.276505\n",
            " 10700/20000: episode: 107, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 333.923828, mae: 113.937119, mean_q: 249.506607\n",
            " 10800/20000: episode: 108, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 333.400146, mae: 114.676132, mean_q: 249.940567\n",
            " 10900/20000: episode: 109, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 228.461716, mae: 114.130280, mean_q: 250.771484\n",
            " 11000/20000: episode: 110, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 376.784027, mae: 114.984871, mean_q: 249.818771\n",
            " 11100/20000: episode: 111, duration: 1.241s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 245.375778, mae: 115.899994, mean_q: 250.826523\n",
            " 11200/20000: episode: 112, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 340.038544, mae: 116.273537, mean_q: 249.053314\n",
            " 11300/20000: episode: 113, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 249.667526, mae: 117.652275, mean_q: 252.126328\n",
            " 11400/20000: episode: 114, duration: 1.197s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 392.189056, mae: 114.977516, mean_q: 250.116974\n",
            " 11500/20000: episode: 115, duration: 1.216s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 276.225006, mae: 112.837608, mean_q: 250.453522\n",
            " 11600/20000: episode: 116, duration: 1.218s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 479.867645, mae: 111.943604, mean_q: 249.236725\n",
            " 11700/20000: episode: 117, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 319.228363, mae: 111.022797, mean_q: 248.955521\n",
            " 11800/20000: episode: 118, duration: 1.248s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 356.269928, mae: 111.881767, mean_q: 248.881714\n",
            " 11900/20000: episode: 119, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 340.455627, mae: 111.856964, mean_q: 249.296478\n",
            " 12000/20000: episode: 120, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 290.344208, mae: 113.147110, mean_q: 249.692688\n",
            " 12100/20000: episode: 121, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 236.709702, mae: 113.504059, mean_q: 248.936890\n",
            " 12200/20000: episode: 122, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 268.433136, mae: 114.934280, mean_q: 250.454025\n",
            " 12300/20000: episode: 123, duration: 1.201s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 341.861572, mae: 114.635162, mean_q: 249.679947\n",
            " 12400/20000: episode: 124, duration: 1.251s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 223.874878, mae: 115.514755, mean_q: 250.937134\n",
            " 12500/20000: episode: 125, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 183.518295, mae: 116.272522, mean_q: 251.252808\n",
            " 12600/20000: episode: 126, duration: 1.216s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 247.765915, mae: 116.284485, mean_q: 251.546585\n",
            " 12700/20000: episode: 127, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 266.216431, mae: 116.480095, mean_q: 251.901077\n",
            " 12800/20000: episode: 128, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 317.158295, mae: 117.536438, mean_q: 251.896500\n",
            " 12900/20000: episode: 129, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 318.231049, mae: 117.778465, mean_q: 251.523865\n",
            " 13000/20000: episode: 130, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 377.781219, mae: 118.490761, mean_q: 251.877625\n",
            " 13100/20000: episode: 131, duration: 1.240s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 328.052063, mae: 117.861046, mean_q: 251.084396\n",
            " 13200/20000: episode: 132, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 358.451416, mae: 116.243332, mean_q: 251.012146\n",
            " 13300/20000: episode: 133, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 387.276489, mae: 114.219383, mean_q: 249.644974\n",
            " 13400/20000: episode: 134, duration: 1.255s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 357.418762, mae: 115.563629, mean_q: 249.825882\n",
            " 13500/20000: episode: 135, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 272.481812, mae: 114.682594, mean_q: 250.400314\n",
            " 13600/20000: episode: 136, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 294.294403, mae: 114.787018, mean_q: 250.741013\n",
            " 13700/20000: episode: 137, duration: 1.214s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 311.944336, mae: 113.907234, mean_q: 250.325012\n",
            " 13800/20000: episode: 138, duration: 1.244s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 312.134399, mae: 112.425262, mean_q: 249.716171\n",
            " 13900/20000: episode: 139, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 253.447617, mae: 112.621346, mean_q: 251.303360\n",
            " 14000/20000: episode: 140, duration: 1.206s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 331.151062, mae: 113.436485, mean_q: 250.733047\n",
            " 14100/20000: episode: 141, duration: 1.184s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 225.697250, mae: 113.863777, mean_q: 250.728699\n",
            " 14200/20000: episode: 142, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 357.940063, mae: 113.345955, mean_q: 249.858490\n",
            " 14300/20000: episode: 143, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 214.168518, mae: 113.829414, mean_q: 251.151031\n",
            " 14400/20000: episode: 144, duration: 1.205s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 364.626068, mae: 114.614189, mean_q: 250.130920\n",
            " 14500/20000: episode: 145, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 283.024170, mae: 115.176369, mean_q: 251.324417\n",
            " 14600/20000: episode: 146, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 288.562408, mae: 114.039726, mean_q: 250.611908\n",
            " 14700/20000: episode: 147, duration: 1.221s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 279.655792, mae: 115.226524, mean_q: 251.100189\n",
            " 14800/20000: episode: 148, duration: 1.210s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 347.700714, mae: 115.258430, mean_q: 251.021835\n",
            " 14900/20000: episode: 149, duration: 1.185s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 356.012115, mae: 114.846794, mean_q: 250.366577\n",
            " 15000/20000: episode: 150, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 330.763397, mae: 114.759659, mean_q: 249.492035\n",
            " 15100/20000: episode: 151, duration: 1.209s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 313.975952, mae: 113.955040, mean_q: 250.537170\n",
            " 15200/20000: episode: 152, duration: 1.235s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 353.791962, mae: 114.286240, mean_q: 249.233688\n",
            " 15300/20000: episode: 153, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 328.722626, mae: 114.856247, mean_q: 248.683533\n",
            " 15400/20000: episode: 154, duration: 1.238s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 260.664276, mae: 115.252869, mean_q: 249.692444\n",
            " 15500/20000: episode: 155, duration: 1.234s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 238.132416, mae: 114.723465, mean_q: 250.140366\n",
            " 15600/20000: episode: 156, duration: 1.266s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 359.719147, mae: 115.396439, mean_q: 249.313354\n",
            " 15700/20000: episode: 157, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 319.570618, mae: 115.707451, mean_q: 249.846771\n",
            " 15800/20000: episode: 158, duration: 1.251s, episode steps: 100, steps per second:  80, episode reward: 497.000, mean reward:  4.970 [ 2.000,  5.000], mean action: 1.980 [0.000, 2.000],  loss: 281.995087, mae: 115.795792, mean_q: 249.489670\n",
            " 15900/20000: episode: 159, duration: 1.193s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 339.302887, mae: 116.018845, mean_q: 249.166946\n",
            " 16000/20000: episode: 160, duration: 1.258s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 406.148560, mae: 114.493721, mean_q: 247.723694\n",
            " 16100/20000: episode: 161, duration: 1.249s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 374.778625, mae: 114.490105, mean_q: 247.828323\n",
            " 16200/20000: episode: 162, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 309.823517, mae: 113.980194, mean_q: 247.146133\n",
            " 16300/20000: episode: 163, duration: 1.219s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 259.159424, mae: 111.714363, mean_q: 247.938965\n",
            " 16400/20000: episode: 164, duration: 1.230s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 322.385223, mae: 111.780838, mean_q: 247.445312\n",
            " 16500/20000: episode: 165, duration: 1.190s, episode steps: 100, steps per second:  84, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 254.234924, mae: 112.447899, mean_q: 248.437241\n",
            " 16600/20000: episode: 166, duration: 1.213s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 221.411606, mae: 113.032990, mean_q: 249.451126\n",
            " 16700/20000: episode: 167, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 364.215637, mae: 112.071297, mean_q: 248.566055\n",
            " 16800/20000: episode: 168, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 309.394409, mae: 109.659943, mean_q: 247.977356\n",
            " 16900/20000: episode: 169, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 264.297577, mae: 111.219902, mean_q: 249.031693\n",
            " 17000/20000: episode: 170, duration: 1.202s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 257.976715, mae: 112.615120, mean_q: 249.646469\n",
            " 17200/20000: episode: 172, duration: 1.207s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 242.366089, mae: 114.462334, mean_q: 250.207382\n",
            " 17300/20000: episode: 173, duration: 1.212s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 335.779846, mae: 114.269241, mean_q: 249.411224\n",
            " 17400/20000: episode: 174, duration: 1.208s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 286.897491, mae: 113.516464, mean_q: 248.752090\n",
            " 17500/20000: episode: 175, duration: 1.237s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 308.513977, mae: 114.043831, mean_q: 248.981522\n",
            " 17600/20000: episode: 176, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 355.659851, mae: 114.369316, mean_q: 248.496796\n",
            " 17700/20000: episode: 177, duration: 1.243s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 198.841736, mae: 115.189529, mean_q: 250.080307\n",
            " 17800/20000: episode: 178, duration: 1.219s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 311.431061, mae: 115.342049, mean_q: 249.394577\n",
            " 17900/20000: episode: 179, duration: 1.245s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 259.126770, mae: 116.115257, mean_q: 250.844864\n",
            " 18000/20000: episode: 180, duration: 1.203s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 326.156097, mae: 115.986755, mean_q: 249.423111\n",
            " 18100/20000: episode: 181, duration: 1.227s, episode steps: 100, steps per second:  81, episode reward: 497.000, mean reward:  4.970 [ 2.000,  5.000], mean action: 1.980 [0.000, 2.000],  loss: 250.358826, mae: 116.600273, mean_q: 251.260269\n",
            " 18200/20000: episode: 182, duration: 1.242s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 292.599792, mae: 116.609192, mean_q: 250.854477\n",
            " 18300/20000: episode: 183, duration: 1.269s, episode steps: 100, steps per second:  79, episode reward: 491.000, mean reward:  4.910 [ 2.000,  5.000], mean action: 1.940 [0.000, 2.000],  loss: 284.492310, mae: 116.457672, mean_q: 250.887543\n",
            " 18400/20000: episode: 184, duration: 1.264s, episode steps: 100, steps per second:  79, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 311.930908, mae: 115.111023, mean_q: 250.864655\n",
            " 18500/20000: episode: 185, duration: 1.258s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 325.248444, mae: 115.502121, mean_q: 250.494186\n",
            " 18600/20000: episode: 186, duration: 1.225s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 307.914520, mae: 115.449455, mean_q: 249.902710\n",
            " 18700/20000: episode: 187, duration: 1.216s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 205.032089, mae: 115.811615, mean_q: 250.795074\n",
            " 18800/20000: episode: 188, duration: 1.215s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 298.203705, mae: 116.269226, mean_q: 250.879303\n",
            " 18900/20000: episode: 189, duration: 1.217s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 258.029602, mae: 114.537315, mean_q: 251.647034\n",
            " 19000/20000: episode: 190, duration: 1.204s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 392.468323, mae: 113.146996, mean_q: 249.802109\n",
            " 19100/20000: episode: 191, duration: 1.226s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 366.730377, mae: 112.433167, mean_q: 250.030685\n",
            " 19200/20000: episode: 192, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 354.035828, mae: 112.273048, mean_q: 248.910706\n",
            " 19300/20000: episode: 193, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 271.878113, mae: 111.961273, mean_q: 250.158569\n",
            " 19400/20000: episode: 194, duration: 1.228s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 296.982880, mae: 111.839050, mean_q: 250.279770\n",
            " 19500/20000: episode: 195, duration: 1.242s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 382.931061, mae: 111.373367, mean_q: 249.121323\n",
            " 19600/20000: episode: 196, duration: 1.222s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 200.156509, mae: 112.005280, mean_q: 251.269440\n",
            " 19700/20000: episode: 197, duration: 1.226s, episode steps: 100, steps per second:  82, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 357.732788, mae: 111.498352, mean_q: 249.262344\n",
            " 19800/20000: episode: 198, duration: 1.236s, episode steps: 100, steps per second:  81, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 249.135330, mae: 111.673958, mean_q: 250.878433\n",
            " 19900/20000: episode: 199, duration: 1.247s, episode steps: 100, steps per second:  80, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 377.540466, mae: 112.447929, mean_q: 249.805603\n",
            " 20000/20000: episode: 200, duration: 1.199s, episode steps: 100, steps per second:  83, episode reward: 500.000, mean reward:  5.000 [ 5.000,  5.000], mean action: 2.000 [2.000, 2.000],  loss: 267.406677, mae: 111.836227, mean_q: 250.488678\n",
            "done, took 251.540 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtjM4Hjlv5ev"
      },
      "source": [
        "# dqn.load_weights(\"/content/drive/MyDrive/Colab Notebooks/RL_IoT_FL/dqn.h5f.data-00000-of-00001.h5f\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3PVIQTQludG"
      },
      "source": [
        "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
        "print(np.mean(scores.history['episode_reward']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5LtCmCsrAZp"
      },
      "source": [
        "**Predict using model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "b2PaHRve4IT2",
        "outputId": "17af7daa-084f-4af4-80b6-e26c7e103b24"
      },
      "source": [
        "df = pd.read_json(\"/content/dqn_RL_IoT-v1_log.json\")\n",
        "# csvdata = df.to_csv()\n",
        "# cs = pd.DataFrame(csvdata)\n",
        "episode = df.loc[:,'episode']\n",
        "mean_reward = df.loc[:,'episode_reward']\n",
        "mean_reward\n",
        "episode\n",
        "df"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>mae</th>\n",
              "      <th>mean_q</th>\n",
              "      <th>episode_reward</th>\n",
              "      <th>nb_episode_steps</th>\n",
              "      <th>nb_steps</th>\n",
              "      <th>episode</th>\n",
              "      <th>duration</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.111432</td>\n",
              "      <td>124.998340</td>\n",
              "      <td>261.242375</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>100</td>\n",
              "      <td>0</td>\n",
              "      <td>7.462015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>198.838577</td>\n",
              "      <td>123.718781</td>\n",
              "      <td>256.469482</td>\n",
              "      <td>455</td>\n",
              "      <td>100</td>\n",
              "      <td>200</td>\n",
              "      <td>1</td>\n",
              "      <td>1.211378</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>252.117767</td>\n",
              "      <td>122.723282</td>\n",
              "      <td>257.169250</td>\n",
              "      <td>488</td>\n",
              "      <td>100</td>\n",
              "      <td>300</td>\n",
              "      <td>2</td>\n",
              "      <td>1.234160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>245.438492</td>\n",
              "      <td>120.792572</td>\n",
              "      <td>257.878876</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>400</td>\n",
              "      <td>3</td>\n",
              "      <td>1.224842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>304.119659</td>\n",
              "      <td>118.783646</td>\n",
              "      <td>255.559906</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>500</td>\n",
              "      <td>4</td>\n",
              "      <td>1.217611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>195</th>\n",
              "      <td>200.156555</td>\n",
              "      <td>112.005264</td>\n",
              "      <td>251.269333</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>19600</td>\n",
              "      <td>195</td>\n",
              "      <td>1.222101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>196</th>\n",
              "      <td>357.732727</td>\n",
              "      <td>111.498352</td>\n",
              "      <td>249.262344</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>19700</td>\n",
              "      <td>196</td>\n",
              "      <td>1.226012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>197</th>\n",
              "      <td>249.135406</td>\n",
              "      <td>111.673935</td>\n",
              "      <td>250.878464</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>19800</td>\n",
              "      <td>197</td>\n",
              "      <td>1.235112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>198</th>\n",
              "      <td>377.540436</td>\n",
              "      <td>112.447929</td>\n",
              "      <td>249.805603</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>19900</td>\n",
              "      <td>198</td>\n",
              "      <td>1.246366</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199</th>\n",
              "      <td>267.406555</td>\n",
              "      <td>111.836205</td>\n",
              "      <td>250.488647</td>\n",
              "      <td>500</td>\n",
              "      <td>100</td>\n",
              "      <td>20000</td>\n",
              "      <td>199</td>\n",
              "      <td>1.198759</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           loss         mae      mean_q  ...  nb_steps  episode  duration\n",
              "0      0.111432  124.998340  261.242375  ...       100        0  7.462015\n",
              "1    198.838577  123.718781  256.469482  ...       200        1  1.211378\n",
              "2    252.117767  122.723282  257.169250  ...       300        2  1.234160\n",
              "3    245.438492  120.792572  257.878876  ...       400        3  1.224842\n",
              "4    304.119659  118.783646  255.559906  ...       500        4  1.217611\n",
              "..          ...         ...         ...  ...       ...      ...       ...\n",
              "195  200.156555  112.005264  251.269333  ...     19600      195  1.222101\n",
              "196  357.732727  111.498352  249.262344  ...     19700      196  1.226012\n",
              "197  249.135406  111.673935  250.878464  ...     19800      197  1.235112\n",
              "198  377.540436  112.447929  249.805603  ...     19900      198  1.246366\n",
              "199  267.406555  111.836205  250.488647  ...     20000      199  1.198759\n",
              "\n",
              "[200 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "k7mHB_Xo6XFG",
        "outputId": "e03ab89c-6d09-4c33-e7a0-89e67fcf9805"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(episode,mean_reward)\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('reward')\n",
        "plt.show()"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}